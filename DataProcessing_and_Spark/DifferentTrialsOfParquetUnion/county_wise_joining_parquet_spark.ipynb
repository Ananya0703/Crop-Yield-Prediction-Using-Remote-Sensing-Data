{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# County wise concatenation of parquet files using Spark\n",
    "\n",
    "- We use spark to join our county wise segmented files. \n",
    "- This method is computationally intensive due to the union operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: netCDF4 in c:\\users\\atish\\.conda\\envs\\bda\\lib\\site-packages (1.6.2)\n",
      "Requirement already satisfied: numpy>=1.9 in c:\\users\\atish\\.conda\\envs\\bda\\lib\\site-packages (from netCDF4) (1.26.0)\n",
      "Requirement already satisfied: cftime in c:\\users\\atish\\.conda\\envs\\bda\\lib\\site-packages (from netCDF4) (1.6.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install netCDF4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|hello|\n",
      "+-----+\n",
      "|spark|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession    \n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "df = spark.sql(\"select 'spark' as hello \")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from netCDF4 import Dataset\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "from datetime import datetime, timedelta\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, DateType, FloatType, IntegerType\n",
    "from pyspark.sql import DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading netcdf files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpack(spark, path, lat_location, lon_location, id):\n",
    "    # data = Dataset(path, 'r')\n",
    "    try:\n",
    "        data = Dataset(path, 'r')\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while opening the file: {str(e)}\")\n",
    "        return None  # Return None to indicate failure\n",
    "    variable_name = list(data.variables.keys())[-1]\n",
    "    # Storing the lat and lon data into the variables \n",
    "    lat = data.variables['lat'][:]\n",
    "    lon = data.variables['lon'][:]\n",
    "\n",
    "\n",
    "    # Squared difference of lat and lon \n",
    "    sq_diff_lat = (lat - lat_location)**2\n",
    "    sq_diff_lon = (lon - lon_location)**2\n",
    "\n",
    "    # Identifying the index of the minimum value for lat and lon \n",
    "    min_index_lat = sq_diff_lat.argmin()\n",
    "    min_index_lon = sq_diff_lon.argmin()\n",
    "\n",
    "    feature = data.variables[variable_name]\n",
    "\n",
    "\n",
    "    days = data.variables['day']\n",
    "    start_date = datetime(1900, 1, 1)  # Start date in the 1900 system\n",
    "    dates = [start_date + timedelta(days=int(day)) for day in days]\n",
    "\n",
    "\n",
    "    schema = StructType([\n",
    "            StructField(\"Date\", DateType(), True),\n",
    "            StructField(variable_name, FloatType(), True),\n",
    "            StructField(\"ID\", IntegerType(), True)\n",
    "        ])\n",
    "    # df = pd.DataFrame(columns=['Date', variable_name])\n",
    "    # df['Date'] = dates\n",
    "    data_list = []\n",
    "\n",
    "    dt = np.arange(0, data.variables['day'].size)\n",
    "    for time_index in dt:\n",
    "        # Use numpy.ma.getdata to get unmasked values\n",
    "        feature_values = float(feature[time_index, min_index_lat, min_index_lon])\n",
    "        # data_list.append((dates[time_index], feature_values))\n",
    "        data_list.append((dates[time_index], feature_values, id))  # Include the \"county\" value\n",
    "\n",
    "\n",
    "        # Now, you can assign the unmasked values to the 'Temperature' column\n",
    "        # df.at[time_index, variable_name] = feature_values\n",
    "    \n",
    "    \n",
    "    spark_df = spark.createDataFrame(data_list, schema)\n",
    "\n",
    "    return spark_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "coordinates_df = spark.read \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .load(\"/Volumes/One Touch/BigDataProject-Capstone/dataCollection/county_cooridnates.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- county: string (nullable = true)\n",
      " |-- lat: double (nullable = true)\n",
      " |-- long: double (nullable = true)\n",
      " |-- ID: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "coordinates_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-----------+---+\n",
      "|    county|       lat|       long| ID|\n",
      "+----------+----------+-----------+---+\n",
      "|     adams|39.9787786|-91.2110065|  0|\n",
      "| alexander|37.1801529|-89.3502834|  1|\n",
      "|      bond|38.8630331|-89.4391416|  2|\n",
      "|     boone| 42.321246|-88.8235511|  3|\n",
      "|     brown|39.9498214|-90.7485656|  4|\n",
      "|    bureau|41.4016294|-89.5341179|  5|\n",
      "|   calhoun|39.1397507|-90.6506113|  6|\n",
      "|   carroll|42.0647352|-89.9556785|  7|\n",
      "|      cass|39.9682127| -90.250722|  8|\n",
      "| champaign|40.1164205|-88.2433829|  9|\n",
      "| christian|39.5212598|-89.2829783| 10|\n",
      "|     clark|39.3260541|-87.7838526| 11|\n",
      "|      clay|38.7340694|-88.4910693| 12|\n",
      "|   clinton|38.5896187| -89.420064| 13|\n",
      "|     coles|39.5266741|-88.2184999| 14|\n",
      "|      cook|41.8197385| -87.756525| 15|\n",
      "|  crawford|39.0131644|-87.7291001| 16|\n",
      "|cumberland|39.2744299|-88.2423795| 17|\n",
      "|   de witt|40.1734773|-88.9003733| 18|\n",
      "|   douglas|39.7628415|-88.2170516| 19|\n",
      "+----------+----------+-----------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "coordinates_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/Volumes/One Touch/BigDataProject-Capstone/gridMET/ Minimum Near-Surface Relative Humidity/rmin_1980.nc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"Date\", DateType(), True),\n",
    "    StructField(\"relative_humidity\", FloatType(), True),\n",
    "    StructField(\"ID\", IntegerType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark union for concatenation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vj/yfxnsl5j0_d0fpgmszq345yr0000gn/T/ipykernel_19167/1307681158.py:42: UserWarning: Warning: converting a masked element to nan.\n",
      "  feature_values = float(feature[time_index, min_index_lat, min_index_lon])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 97.86711406707764 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "concatenated_df = None\n",
    "\n",
    "for row in coordinates_df.rdd.collect():\n",
    "    ID = row['ID']\n",
    "    lat_location = row['lat']\n",
    "    lon_location = row['long']\n",
    "    \n",
    "    # Call the unpack function for the current row\n",
    "    result_df = unpack(spark, path, lat_location, lon_location, ID)\n",
    "    \n",
    "    if isinstance(result_df, DataFrame):\n",
    "        if concatenated_df is None:\n",
    "            concatenated_df = result_df\n",
    "        else:\n",
    "            concatenated_df = concatenated_df.unionAll(result_df)\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Execution time: {execution_time} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/10/26 02:38:27 WARN DAGScheduler: Broadcasting large task binary with size 5.6 MiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------+---+\n",
      "|      Date|relative_humidity| ID|\n",
      "+----------+-----------------+---+\n",
      "|1980-01-01|             44.8|  0|\n",
      "|1980-01-02|             40.2|  0|\n",
      "|1980-01-03|             43.7|  0|\n",
      "|1980-01-04|             52.5|  0|\n",
      "|1980-01-05|             55.5|  0|\n",
      "|1980-01-06|             67.8|  0|\n",
      "|1980-01-07|             50.4|  0|\n",
      "|1980-01-08|             56.1|  0|\n",
      "|1980-01-09|             49.1|  0|\n",
      "|1980-01-10|             58.7|  0|\n",
      "|1980-01-11|             46.2|  0|\n",
      "|1980-01-12|             43.8|  0|\n",
      "|1980-01-13|             59.4|  0|\n",
      "|1980-01-14|             55.1|  0|\n",
      "|1980-01-15|             68.1|  0|\n",
      "|1980-01-16|             60.3|  0|\n",
      "|1980-01-17|             59.7|  0|\n",
      "|1980-01-18|             61.1|  0|\n",
      "|1980-01-19|             58.9|  0|\n",
      "|1980-01-20|             54.1|  0|\n",
      "+----------+-----------------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "concatenated_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/10/26 02:07:44 WARN DAGScheduler: Broadcasting large task binary with size 5.0 MiB\n",
      "23/10/26 02:11:52 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "694"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concatenated_df.select('county').distinct().count()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
